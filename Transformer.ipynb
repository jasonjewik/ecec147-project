{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bede51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import collections\n",
    "import datetime\n",
    "import glob\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "# Third party imports\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SAMPLING_RATE = 16000\n",
    "midi_dir = 'maestro-v3.0.0'\n",
    "npy_dir = 'maestro-numpy'\n",
    "filenames = glob.glob(f'{midi_dir}/**/*.mid*')\n",
    "\n",
    "sample_file = filenames[1]\n",
    "pm = pretty_midi.PrettyMIDI(sample_file)\n",
    "\n",
    "def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):\n",
    "    waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n",
    "    waveform_short = waveform[:seconds*_SAMPLING_RATE]\n",
    "    return display.Audio(waveform_short, rate=_SAMPLING_RATE)\n",
    "\n",
    "display_audio(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09bc3f",
   "metadata": {},
   "source": [
    "# MIDI to NumPy\n",
    "Extract sequences from the MIDI files and store them as NumPy arrays in npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128\n",
    "\n",
    "def midi_to_notes(midi_file):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_file)\n",
    "    instrument = pm.instruments[0]\n",
    "    notes = collections.defaultdict(list)\n",
    "\n",
    "    sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
    "    prev_start = sorted_notes[0].start\n",
    "\n",
    "    for note in sorted_notes:\n",
    "        start = note.start\n",
    "        end = note.end\n",
    "        notes['pitch'].append(note.pitch)\n",
    "        notes['start'].append(start)\n",
    "        notes['end'].append(end)\n",
    "        notes['step'].append(start - prev_start)\n",
    "        notes['duration'].append(end - start)\n",
    "        prev_start = start\n",
    "\n",
    "    return pd.DataFrame({name: np.array(value) for name, value in notes.items()})\n",
    "\n",
    "def get_sequences(seq_length):\n",
    "    all_notes = []\n",
    "    for f in filenames:\n",
    "        notes = midi_to_notes(f)\n",
    "        all_notes.append(notes)\n",
    "    all_notes = pd.concat(all_notes)\n",
    "    key_order = ['pitch', 'step', 'duration']\n",
    "    data = np.stack([all_notes[key] for key in key_order], axis=1)\n",
    "    data = torch.tensor(data)\n",
    "    sequences = data.unfold(0, seq_length + 1, 1)\n",
    "\n",
    "    inputs = sequences[:, :, :-1].numpy()\n",
    "    labels = sequences[:, :, -1].numpy()\n",
    "    return inputs, labels\n",
    "\n",
    "def create_npy_datasets(seq_length):\n",
    "    inputs_file = os.path.join(npy_dir, f'inputs.{seq_length}.npy')\n",
    "    labels_file = os.path.join(npy_dir, f'labels.{seq_length}.npy')\n",
    "    if os.path.exists(inputs_file) and os.path.exists(labels_file):\n",
    "        print('npy datasets already exist, delete to re-generate... quitting')\n",
    "        return\n",
    "    if os.path.exists(inputs_file):\n",
    "        os.unlink(inputs_file)\n",
    "    if os.path.exists(labels_file):\n",
    "        os.unlink(labels_file)\n",
    "    if not os.path.exists(npy_dir):\n",
    "        os.mkdir(npy_dir)\n",
    "    inputs, labels = get_sequences(seq_length)\n",
    "    with open(inputs_file, 'wb') as f:\n",
    "        np.save(f, inputs)\n",
    "    with open(labels_file, 'wb') as f:\n",
    "        np.save(f, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387acb69",
   "metadata": {},
   "source": [
    "Create two datasets, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c82509",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_npy_datasets(seq_length=16)\n",
    "create_npy_datasets(seq_length=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e113f",
   "metadata": {},
   "source": [
    "# PyTorch Dataset and Dataloader\n",
    "Loads the extract sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab_size, split='train'):\n",
    "        train_end = val_start = round(X.shape[0] * 0.8)\n",
    "        val_end = test_start = round(y.shape[0] * 0.9)\n",
    "        if split == 'train':\n",
    "            X = X[:train_end]\n",
    "            y = y[:train_end]\n",
    "        elif split == 'val':\n",
    "            X = X[val_start:val_end]\n",
    "            y = y[val_start:val_end]\n",
    "        elif split == 'test':\n",
    "            X = X[test_start:]\n",
    "            y = y[test_start:]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        self.X = torch.tensor(X, device=device)\n",
    "        self.y = torch.tensor(y, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X, y out here instead of in the dataset class to save memory/time\n",
    "with open(os.path.join(npy_dir, 'inputs.16.npy'), 'rb') as f:\n",
    "    X = np.load(f)\n",
    "with open(os.path.join(npy_dir, 'labels.16.npy'), 'rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct datasets and dataloaders\n",
    "train_ds = MusicDataset(X, y, vocab_size, split='train')\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_ds = MusicDataset(X, y, vocab_size, split='val')\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "# Free up memory\n",
    "del X; del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c7e70",
   "metadata": {},
   "source": [
    "Convert tensor back to playable audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2519c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_to_midi(notes, out_file=None, instr_name='Acoustic Grand Piano', velocity=100):\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    instr = pretty_midi.Instrument(\n",
    "        program=pretty_midi.instrument_name_to_program(instr_name)\n",
    "    )\n",
    "    prev_start = 0\n",
    "    for i, note in notes.iterrows():\n",
    "        start = float(prev_start + note['step'])\n",
    "        end = float(start + note['duration'])\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=velocity,\n",
    "            pitch=int(note['pitch']),\n",
    "            start=start,\n",
    "            end=end\n",
    "        )\n",
    "        instr.notes.append(note)\n",
    "        prev_start = start\n",
    "\n",
    "    pm.instruments.append(instr)\n",
    "    if out_file:\n",
    "        pm.write(out_file)\n",
    "    return pm\n",
    "\n",
    "def tensor_to_midi(tensor):\n",
    "    prev_start = 0\n",
    "    notes = []\n",
    "    for i in range(tensor.shape[1]):\n",
    "        pitch, step, duration = tensor[:, i]\n",
    "        pitch = pitch.item()\n",
    "        step = step.item()\n",
    "        duration = duration.item()\n",
    "        start = prev_start + step\n",
    "        end = start + duration\n",
    "        note = (pitch, step, duration)\n",
    "        notes.append((*note, start, end))\n",
    "        prev_start = start\n",
    "    notes = pd.DataFrame(notes, columns=('pitch', 'step', 'duration', 'start', 'end'))\n",
    "    pm = notes_to_midi(notes)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = tensor_to_midi(train_ds[1][0])\n",
    "display_audio(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4eed5",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, seq_length, vocab_size, \n",
    "                 num_decoder_layers, num_hidden_fc_layers, \n",
    "                 transformer_dropout, device):\n",
    "        super().__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=3, nhead=3, dropout=transformer_dropout, \n",
    "            batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.decoder = self.decoder.to(device=device).double()\n",
    "        self.flatten = nn.Flatten()\n",
    "        if num_hidden_fc_layers == 1:\n",
    "            self.fcnn = nn.Sequential(\n",
    "                nn.Linear(seq_length * 3, vocab_size + 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(vocab_size + 2, vocab_size + 2)\n",
    "            )\n",
    "        elif num_hidden_fc_layers == 2:\n",
    "            self.fcnn = nn.Sequential(\n",
    "                nn.Linear(seq_length * 3, vocab_size + 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(vocab_size + 2, vocab_size + 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(vocab_size + 2, vocab_size + 2)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.fcnn = self.fcnn.to(device=device).double()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        z = self.decoder(x, y)\n",
    "        z = self.flatten(z)\n",
    "        z = self.fcnn(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25804098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicTransformer(\n",
    "    seq_length=16,\n",
    "    vocab_size=vocab_size,\n",
    "    num_decoder_layers=3,\n",
    "    num_hidden_fc_layers=1,\n",
    "    transformer_dropout=0.1,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24f5fb",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dl:\n",
    "    break\n",
    "x = x.moveaxis(-1, 1)\n",
    "y = y.unsqueeze(1)\n",
    "print(x.shape, y.shape)\n",
    "with torch.no_grad():\n",
    "    result = model(x, y)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d3764",
   "metadata": {},
   "source": [
    "Functions to get a MIDI file from model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_notes(x, y, model, temperature=0.1):\n",
    "    x = x.unsqueeze(0).moveaxis(-1, 1)\n",
    "    y = y.unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        preds = model(x, y)\n",
    "    pitch_logits = preds[:, :vocab_size]\n",
    "    if np.random.random() <= temperature:\n",
    "        pitch = torch.randint(low=0, high=vocab_size+1, size=(1,))\n",
    "    else:\n",
    "        pitch = torch.argmax(F.softmax(pitch_logits, dim=0))\n",
    "    step = preds[:, -2]\n",
    "    duration = preds[:, -1]\n",
    "    return pitch.item(), step.item(), duration.item()\n",
    "\n",
    "def generate_sequence(num_preds, x, y, model, temperature):\n",
    "    generated_notes = []\n",
    "    prev_start = 0\n",
    "    input_notes = x\n",
    "    seq_length = input_notes.shape[1]\n",
    "    for _ in range(num_preds):\n",
    "        pitch, step, duration = predict_next_notes(x, y, model, temperature)\n",
    "        start = prev_start + step\n",
    "        end = start + duration\n",
    "        input_note = (pitch, step, duration)\n",
    "        generated_notes.append((*input_note, start, end))\n",
    "        input_notes = input_notes[:, 1:]\n",
    "        input_note = torch.tensor(input_note) / torch.tensor([seq_length, 1, 1])\n",
    "        input_note = input_note.to(device=device).unsqueeze(1)\n",
    "        input_notes = torch.cat((input_notes, input_note), dim=1)\n",
    "        prev_start = start\n",
    "    \n",
    "    generated_notes = pd.DataFrame(generated_notes, columns=('pitch', 'step', 'duration', 'start', 'end'))\n",
    "    pm = notes_to_midi(generated_notes)\n",
    "    return pm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f46c5",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate_sequence(120, *train_ds[0], model, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada35ce4",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_criterion = nn.MSELoss()\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "loss_weights = {'ce': 0.5, 'mse': 0.5}\n",
    "\n",
    "# Get current datetime\n",
    "def current_time():\n",
    "    return datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Custom loss function\n",
    "def criterion(preds, true, weights):\n",
    "    true = true.squeeze()\n",
    "    ce_loss = ce_criterion(preds[:, :vocab_size], true[:, 0].long())\n",
    "    ce_loss *= weights['ce']\n",
    "    mse_loss = mse_criterion(preds[:, vocab_size:], true[:, 1:])\n",
    "    mse_loss *= weights['mse']\n",
    "    return ce_loss + mse_loss\n",
    "\n",
    "# Gets loss over a validation/test dataloader\n",
    "def evaluate_loss(model, test_dl, loss_weights):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(test_dl):\n",
    "            x = x.moveaxis(-1, 1)\n",
    "            y = y.unsqueeze(1)\n",
    "            outputs = model(x, y)\n",
    "            loss = criterion(outputs, y, loss_weights)\n",
    "            test_loss += loss.item()\n",
    "    test_loss /= (i + 1)\n",
    "    model.train()\n",
    "    return test_loss\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_dl, valid_dl, loss_weights, num_epochs, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_loss = 9999\n",
    "    stop = False\n",
    "    model_path = ''\n",
    "    model_dir = 'models'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    model.train()\n",
    "    \n",
    "    print('Started training')\n",
    "    for epoch in range(num_epochs):\n",
    "        if stop:\n",
    "            break\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_dl):  \n",
    "            optimizer.zero_grad()\n",
    "            x = x.moveaxis(-1, 1)\n",
    "            y = y.unsqueeze(1)\n",
    "            outputs = model(x, y)\n",
    "            loss = criterion(outputs, y, loss_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] train loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "                \n",
    "            if i % 8000 == 7999:  # print every 8000 mini-batches\n",
    "                val_loss = evaluate_loss(model, valid_dl, loss_weights)\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] val loss: {val_loss:.3f}')\n",
    "                timestamp = current_time()\n",
    "                seq_length = x.shape[1]\n",
    "                fname = f'music_transformer.{timestamp}.{seq_length}.pt'\n",
    "                torch.save(model.state_dict(), fname)\n",
    "                print(f'Saved model to {os.path.join(model_dir, fname)}')\n",
    "                if val_loss < best_val_loss:\n",
    "                    print('new best!')\n",
    "                    best_val_loss = val_loss\n",
    "                    model_path = fname\n",
    "                if val_loss > best_val_loss:\n",
    "                    print('model performance decreasing... stopping early')\n",
    "                    stop = True\n",
    "                    break\n",
    "                    \n",
    "    print('Finished training')\n",
    "    return model_path, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f8c1d",
   "metadata": {},
   "source": [
    "We try the following hyperparameter settings:\n",
    "- Input sequence length: 16 vs. 32\n",
    "- Number of transformer decoder layers: 3 vs. 4\n",
    "- Number of hidden FC layers: 1 vs. 2\n",
    "- Dropout in the transformer layers: 0.1 vs. 0.3\n",
    "- Learning rate: $1^{-3}$ vs. $1^{-4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "with open(os.path.join(npy_dir, 'inputs.16.npy'), 'rb') as f:\n",
    "    X = np.load(f)\n",
    "with open(os.path.join(npy_dir, 'labels.16.npy'), 'rb') as f:\n",
    "    y = np.load(f)\n",
    "\n",
    "train_ds = MusicDataset(X, y, vocab_size, split='train')\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_ds = MusicDataset(X, y, vocab_size, split='val')\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "del X; del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with length-16 inputs\n",
    "hparam_settings = []\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "for tup in product([3,4],[1,2],[0.1,0.3]):\n",
    "    setting = dict([\n",
    "        ('num_decoder_layers', tup[0]),\n",
    "        ('num_hidden_fc_layers', tup[1]),\n",
    "        ('transformer_dropout', tup[2])\n",
    "    ])\n",
    "    hparam_settings.append(setting)\n",
    "\n",
    "best16_val_loss = 9999\n",
    "best16_model_path = ''\n",
    "best16_hparams = {}\n",
    "for lr in learning_rates:\n",
    "    for hparams in hparam_settings:\n",
    "        print(f'Learning rate = {lr}')\n",
    "        print(hparams)\n",
    "        model = MusicTransformer(    \n",
    "            **hparams,\n",
    "            seq_length=16,\n",
    "            vocab_size=vocab_size,\n",
    "            device=device\n",
    "        )\n",
    "        path, val_loss = train(\n",
    "            model, \n",
    "            train_dl, \n",
    "            val_dl, \n",
    "            loss_weights, \n",
    "            num_epochs=1,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        if val_loss < best16_val_loss:\n",
    "            best16_model_path = path\n",
    "            best16_val_loss = val_loss\n",
    "            best16_hparams = hparams\n",
    "            best16_hparams['lr'] = lr\n",
    "            \n",
    "print(f'Best validation loss: {best16_val_loss}')\n",
    "print(f'Hyper parameter settings: {best16_hparams}')\n",
    "print(f'Path to best model: {best16_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff3547",
   "metadata": {},
   "source": [
    "Now the length-32 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "with open(os.path.join(npy_dir, 'inputs.32.npy'), 'rb') as f:\n",
    "    X = np.load(f)\n",
    "with open(os.path.join(npy_dir, 'labels.32.npy'), 'rb') as f:\n",
    "    y = np.load(f)\n",
    "\n",
    "train_ds = MusicDataset(X, y, vocab_size, split='train')\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_ds = MusicDataset(X, y, vocab_size, split='val')\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "del X; del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed973096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with length-16 inputs\n",
    "hparam_settings = []\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "for tup in product([3,4],[1,2],[0.1,0.3]):\n",
    "    setting = dict([\n",
    "        ('num_decoder_layers', tup[0]),\n",
    "        ('num_hidden_fc_layers', tup[1]),\n",
    "        ('transformer_dropout', tup[2])\n",
    "    ])\n",
    "    hparam_settings.append(setting)\n",
    "\n",
    "best32_val_loss = 9999\n",
    "best32_model_path = ''\n",
    "best32_hparams = {}\n",
    "for lr in learning_rates:\n",
    "    for hparams in hparam_settings:\n",
    "        print(f'Learning rate = {lr}')\n",
    "        print(hparams)\n",
    "        model = MusicTransformer(    \n",
    "            **hparams,\n",
    "            seq_length=32,\n",
    "            vocab_size=vocab_size,\n",
    "            device=device\n",
    "        )\n",
    "        path, val_loss = train(\n",
    "            model, \n",
    "            train_dl, \n",
    "            val_dl, \n",
    "            loss_weights, \n",
    "            num_epochs=1,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        if val_loss < best32_val_loss:\n",
    "            best32_model_path = path\n",
    "            best32_val_loss = val_loss\n",
    "            best32_hparams = hparams\n",
    "            best32_hparams['lr'] = lr\n",
    "            \n",
    "print(f'Best validation loss: {best32_val_loss}')\n",
    "print(f'Hyper parameter settings: {best32_hparams}')\n",
    "print(f'Path to best model: {best32_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2dcd8",
   "metadata": {},
   "source": [
    "# Evaluate on test data\n",
    "Check testing loss and generate some music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a33939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load length-16 testing dataset\n",
    "with open(os.path.join(npy_dir, 'inputs.16.npy'), 'rb') as f:\n",
    "    X = np.load(f)\n",
    "with open(os.path.join(npy_dir, 'labels.16.npy'), 'rb') as f:\n",
    "    y = np.load(f)\n",
    "    \n",
    "test_ds = MusicDataset(X, y, vocab_size=vocab_size, split='test')\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "del X; del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c750ca3",
   "metadata": {},
   "source": [
    "Evaluate the model trained on length-16 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26148912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best16_hparams.pop('lr')  # don't pass learning rate to the model\n",
    "model = MusicTransformer(    \n",
    "    **best16_hparams,\n",
    "    seq_length=16,\n",
    "    vocab_size=vocab_size,\n",
    "    device=device\n",
    ")\n",
    "if os.path.exists(best16_model_path):\n",
    "    model.load_state_dict(torch.load(best16_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0533ffa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_loss(model, test_dl, loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2f474",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm = generate_sequence(120, *test_ds[0], model, temperature=0.3)\n",
    "display_audio(pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47b72f",
   "metadata": {},
   "source": [
    "Evaluate the model trained on length-32 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load length-32 testing dataset\n",
    "with open(os.path.join(npy_dir, 'inputs.32.npy'), 'rb') as f:\n",
    "    X = np.load(f)\n",
    "with open(os.path.join(npy_dir, 'labels.32.npy'), 'rb') as f:\n",
    "    y = np.load(f)\n",
    "    \n",
    "test_ds = MusicDataset(X, y, vocab_size=vocab_size, split='test')\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "del X; del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e7d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best32_hparams.pop('lr')  # don't pass learning rate to the model\n",
    "model = MusicTransformer(    \n",
    "    **best32_hparams,\n",
    "    seq_length=32,\n",
    "    vocab_size=vocab_size,\n",
    "    device=device\n",
    ")\n",
    "if os.path.exists(best32_model_path):\n",
    "    model.load_state_dict(torch.load(best32_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_loss(model, test_dl, loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507f999",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm = generate_sequence(120, *test_ds[0], model, temperature=0.3)\n",
    "display_audio(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcd9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
